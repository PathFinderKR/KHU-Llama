{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "433ddd1e8028bb07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Huggingface\n",
    "import huggingface_hub\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Unsloth\n",
    "from unsloth import FastLanguageModel, is_bf16_supported, UnslothTrainer, UnslothTrainingArguments"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "d35dfb5dcffb629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    model_id: str = \"meta-llama/Llama-3.2-11B\"\n",
    "    \n",
    "    # HuggingFace Hub\n",
    "    username: str = \"PathFinderKR\"\n",
    "    model_name: str = f\"KHU-Llama-3.2-11B\"\n",
    "    \n",
    "    # Data\n",
    "    dataset_id: str = \"Khudanlp/khu_data\"\n",
    "    \n",
    "    # Training\n",
    "    ## Paths\n",
    "    output_dir: str = \"./results\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 10\n",
    "    save_total_limit: int = 1\n",
    "    report_to: str = \"wandb\" if not debug else None\n",
    "    ## Hyperparameters\n",
    "    num_train_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    fp16: bool = not is_bf16_supported()\n",
    "    bf16: bool = is_bf16_supported()\n",
    "    dtype: torch.dtype = torch.bfloat16 if is_bf16_supported() else torch.float16\n",
    "    load_in_4bit: bool = True\n",
    "    learning_rate: float = 5e-5\n",
    "    embedding_learning_rate = 1e-5\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    optim: str = \"adamw_8bit\"\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_length: int = 2048\n",
    "    dataset_num_proc: int = 2\n",
    "    packing: bool = True\n",
    "    ### LoRA\n",
    "    lora: bool = True\n",
    "    if lora:\n",
    "        r: int = 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"]\n",
    "        lora_alpha: int = 32\n",
    "        lora_dropout: float = 0\n",
    "        bias: str = \"none\"\n",
    "        use_gradient_checkpointing: str = \"unsloth\"\n",
    "        use_rslora: bool = True\n",
    "        loftq_config: str = None\n",
    "        save_method: str = \"merged_16bit\"\n",
    "    \n",
    "    # Inference\n",
    "    max_new_tokens: int = 2048\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    repetition_penalty: float = 1.1\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42"
   ],
   "id": "93281c231467f7e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "5525bda12104bb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "c7f89213fe1e3b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "7844e5ceef3bb8a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "b24f8a736bfe545f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Debugging",
   "id": "8fadfa0dd0dd4538"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.num_train_epochs = 1"
   ],
   "id": "214b07b072f88149",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HuggingFace",
   "id": "c148b23c553f1ff3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "huggingface_hub.login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "id": "67bd0191d59a818f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weights & Biases",
   "id": "bd1cd43b2bdb24d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.login(\n",
    "        key=os.getenv(\"WANDB_API_KEY\")\n",
    "    )\n",
    "    wandb.init(\n",
    "        project=CONFIG.model_name\n",
    "    )"
   ],
   "id": "9c1d79090b69737f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utility Functions",
   "id": "295cb5ac174c88b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate base model\n",
    "def generate_text(prompt):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ],
   "id": "9367124b154dabf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "khu_prompt = \"\"\"Kyung Hee University\n",
    "### Title: {}\n",
    "\n",
    "### Information:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "def formatting_func(examples):\n",
    "    texts = []\n",
    "    metadata = examples[\"META\"]\n",
    "    data = examples[\"TEXT\"]\n",
    "    for metadata, data in zip(metadata, data):\n",
    "        text = khu_prompt.format(metadata, data) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ],
   "id": "e8cbded2937589b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_token_length(fields):\n",
    "    for field in fields:\n",
    "        token_lengths = [len(tokenizer.encode(example[field])) for example in dataset if example[field] != \"\"]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel(f'{field.capitalize()} Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{field.capitalize()} Token Length Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Max {field} token length: {max(token_lengths)}\")\n",
    "        print(f\"Min {field} token length: {min(token_lengths)}\")\n",
    "        print(f\"Mean {field} token length: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"Standard deviation of {field} token length: {np.std(token_lengths):.2f}\")"
   ],
   "id": "da41a5bf2f6a36f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d82f87f74b3d94e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG.model_id,\n",
    "    max_seq_length=CONFIG.max_seq_length,\n",
    "    dtype=CONFIG.dtype,\n",
    "    load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    ")"
   ],
   "id": "cb1e60af7efcb450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.all_special_tokens}\")"
   ],
   "id": "2630dd4cb3663ac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ],
   "id": "e7176bb1ccdeeca4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    sample_text = \"Kyung Hee University:\"\n",
    "    sample_response = generate_text(sample_text)\n",
    "    print(sample_response)"
   ],
   "id": "aa3bb022dea115b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "7632317c22989499"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = load_dataset(CONFIG.dataset_id, split=\"train\")",
   "id": "f6f8bf4732ed38ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "63f155605ee4c12c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[0][\"TEXT\"])"
   ],
   "id": "50e594b242b76d34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "333975d5c50820a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = dataset.map(formatting_func, batched=True)",
   "id": "898878d6328ab66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[0][\"text\"])\n",
    "    print(tokenizer.tokenize(dataset[0][\"text\"]))"
   ],
   "id": "eb35ca6533c912b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    plot_token_length([\"text\"])"
   ],
   "id": "e2569cf935d78df9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Supervised Fine-Tuning",
   "id": "a9029bed5c49bee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=CONFIG.r,\n",
    "        target_modules=CONFIG.target_modules,\n",
    "        lora_alpha=CONFIG.lora_alpha,\n",
    "        lora_dropout=CONFIG.lora_dropout,\n",
    "        bias=CONFIG.bias,\n",
    "        use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "        use_rslora=CONFIG.use_rslora,\n",
    "        loftq_config=CONFIG.loftq_config,\n",
    "        random_state=CONFIG.seed\n",
    "    )"
   ],
   "id": "41dbb68405aa52b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    model.print_trainable_parameters()"
   ],
   "id": "f005f3823824359e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG.max_seq_length,\n",
    "    dataset_num_proc=CONFIG.dataset_num_proc,\n",
    "    packing=CONFIG.packing,\n",
    "    args=UnslothTrainingArguments(\n",
    "        output_dir=CONFIG.output_dir,\n",
    "        logging_dir=CONFIG.logging_dir,\n",
    "        save_strategy=CONFIG.save_strategy,\n",
    "        logging_strategy=CONFIG.logging_strategy,\n",
    "        logging_steps=CONFIG.logging_steps,\n",
    "        save_total_limit=CONFIG.save_total_limit,\n",
    "        report_to=CONFIG.report_to,\n",
    "        num_train_epochs=CONFIG.num_train_epochs,\n",
    "        per_device_train_batch_size=CONFIG.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n",
    "        fp16=CONFIG.fp16,\n",
    "        bf16=CONFIG.bf16,\n",
    "        learning_rate=CONFIG.learning_rate,\n",
    "        embedding_learning_rate=CONFIG.embedding_learning_rate,\n",
    "        lr_scheduler_type=CONFIG.lr_scheduler_type,\n",
    "        warmup_ratio=CONFIG.warmup_ratio,\n",
    "        optim=CONFIG.optim,\n",
    "        weight_decay=CONFIG.weight_decay\n",
    "    )\n",
    ")"
   ],
   "id": "d2d9843552b6ff26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "fef742b59ae750cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.finish()\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "        tokenizer.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "    else:\n",
    "        model.save_pretrained(CONFIG.model_name)\n",
    "        tokenizer.save_pretrained(CONFIG.model_name)"
   ],
   "id": "81f595ecc945c59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "e912d53f6aa9676e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_text = \"Kyung Hee University:\"\n",
    "sample_response = generate_text(sample_text)"
   ],
   "id": "2fc3203546fc7644"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save",
   "id": "9b5cf1b4ed3708c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "        model.push_to_hub_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "    else:\n",
    "        model.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )"
   ],
   "id": "a4c00c867c11af7d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
