{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "433ddd1e8028bb07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:56.547065Z",
     "start_time": "2024-11-03T08:06:52.576558Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Huggingface\n",
    "import huggingface_hub\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameters",
   "id": "d35dfb5dcffb629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:56.552829Z",
     "start_time": "2024-11-03T08:06:56.548139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    model_type: str = \"base\"  # \"base\", \"instruct\"\n",
    "    model_size: str = \"1B\"  # \"1B\", \"3B\"\n",
    "    if model_type == \"base\":\n",
    "        if model_size == \"1B\":\n",
    "            model_id: str = \"meta-llama/Llama-3.2-1B\"\n",
    "        elif model_size == \"3B\":\n",
    "            model_id: str = \"meta-llama/Llama-3.2-3B\"\n",
    "    elif model_type == \"instruct\":\n",
    "        if model_size == \"1B\":\n",
    "            model_id: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "        elif model_size == \"3B\":\n",
    "            model_id: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "    # HuggingFace Hub\n",
    "    username: str = \"PathFinderKR\"\n",
    "    model_name: str = f\"Llama-3.2-KO-{model_size}-Instruct\"\n",
    "    repo_id: str = f\"{username}/{model_name}\"\n",
    "    \n",
    "    # Data\n",
    "    dataset_id: str = \"MarkrAI/KOpen-HQ-Hermes-2.5-60K\"\n",
    "    validation_size: float = 0.1\n",
    "    \n",
    "    # Training\n",
    "    output_dir: str = \"./results\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 10\n",
    "    evaluation_strategy: str = \"epoch\"\n",
    "    save_total_limit: int = 1\n",
    "    report_to: str = \"wandb\" if not debug else None\n",
    "    \n",
    "    num_train_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    gradient_checkpointing: bool = True\n",
    "    bf16: bool = True\n",
    "    learning_rate: float = 2e-5\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    optim: str = \"adamw_torch\"\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_length: int = 4086\n",
    "    \n",
    "    # Inference\n",
    "    max_new_tokens: int = 128000\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    repetition_penalty: float = 1.1\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    attn_implementation: str = None\n",
    "    torch_dtype: torch.dtype = torch.bfloat16\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42"
   ],
   "id": "93281c231467f7e7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reproducibility",
   "id": "5525bda12104bb7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:56.568469Z",
     "start_time": "2024-11-03T08:06:56.553797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "c7f89213fe1e3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "7844e5ceef3bb8a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:56.729056Z",
     "start_time": "2024-11-03T08:06:56.569471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "b24f8a736bfe545f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:56.733365Z",
     "start_time": "2024-11-03T08:06:56.730421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_attn_implementation(device):\n",
    "    if device == \"cuda\":\n",
    "        if torch.cuda.get_device_capability()[0] >= 8: # Ampere, Ada, or Hopper GPUs\n",
    "            attn_implementation = \"flash_attention_2\"\n",
    "        else:\n",
    "            attn_implementation = \"eager\"\n",
    "    else:\n",
    "        attn_implementation = None\n",
    "    return attn_implementation\n",
    "\n",
    "CONFIG.attn_implementation= configure_attn_implementation(CONFIG.device)"
   ],
   "id": "70ddd7a9cf3d7b6b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debugging",
   "id": "8fadfa0dd0dd4538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:56.740319Z",
     "start_time": "2024-11-03T08:06:56.734389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.num_train_epochs = 1"
   ],
   "id": "214b07b072f88149",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HuggingFace",
   "id": "c148b23c553f1ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:57.026748Z",
     "start_time": "2024-11-03T08:06:56.741438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "huggingface_hub.login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "id": "67bd0191d59a818f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/pathfinder/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Weights & Biases",
   "id": "bd1cd43b2bdb24d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:58.661105Z",
     "start_time": "2024-11-03T08:06:57.027734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.login(\n",
    "        key=os.getenv(\"WANDB_API_KEY\")\n",
    "    )\n",
    "    wandb.init(\n",
    "        project=CONFIG.model_name,\n",
    "    )"
   ],
   "id": "9c1d79090b69737f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/cheir/GitHub/KHU-Llama/wandb/run-20241103_170658-0qeod7ij</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/Llama-3.2-KO-1B-Instruct/runs/0qeod7ij' target=\"_blank\">confused-bush-1</a></strong> to <a href='https://wandb.ai/pathfinderkr/Llama-3.2-KO-1B-Instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/Llama-3.2-KO-1B-Instruct' target=\"_blank\">https://wandb.ai/pathfinderkr/Llama-3.2-KO-1B-Instruct</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/Llama-3.2-KO-1B-Instruct/runs/0qeod7ij' target=\"_blank\">https://wandb.ai/pathfinderkr/Llama-3.2-KO-1B-Instruct/runs/0qeod7ij</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utility Functions",
   "id": "63779c0cf1a2b825"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:58.664890Z",
     "start_time": "2024-11-03T08:06:58.662122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_base_model(prompt):\n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(CONFIG.device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        streamer=streamer\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)"
   ],
   "id": "b98855737cec263f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:58.674589Z",
     "start_time": "2024-11-03T08:06:58.665847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Llama-3-Instruct template\n",
    "def prompt_template(system, user):\n",
    "    return (\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"{system}<|eot_id|>\"\n",
    "        \n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{user}<|eot_id|>\"\n",
    "        \n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "\n",
    "def generate_instruct_model(system, user):\n",
    "    prompt = prompt_template(system, user)\n",
    "    \n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(CONFIG.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        streamer=streamer\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ],
   "id": "6d3703a363802674",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:58.681912Z",
     "start_time": "2024-11-03T08:06:58.675875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alpaca dataset format: \n",
    "# {\"input\": [str],\n",
    "#  \"instruction\": [str],\n",
    "#   \"output\": [str]}\n",
    "\n",
    "def prompt_without_input(example):\n",
    "    text = (\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{example['instruction']}<|eot_id|>\"\n",
    "        \n",
    "        f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        f\"{example['output']}\"\n",
    "    )\n",
    "    return {'text': text}\n",
    "    \n",
    "def prompt_with_input(example):\n",
    "    text = (\n",
    "        f\"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        f\"{example['input']}<|eot_id|>\"\n",
    "        \n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{example['instruction']}<|eot_id|>\"\n",
    "        \n",
    "        f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        f\"{example['output']}\"\n",
    "    )\n",
    "    return {'text': text}\n",
    "    \n",
    "def formatting_func(example):\n",
    "    # if input is not provided\n",
    "    if example[\"input\"] == \"\":\n",
    "        return prompt_without_input(example)\n",
    "    # if input is provided\n",
    "    else:\n",
    "        return prompt_with_input(example)"
   ],
   "id": "8708e28355cc0e05",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:58.692084Z",
     "start_time": "2024-11-03T08:06:58.682938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_token_length(fields):\n",
    "    for field in fields:\n",
    "        token_lengths = [len(tokenizer.encode(example[field])) for example in dataset[\"train\"] if example[field] != \"\"]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel(f'{field.capitalize()} Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{field.capitalize()} Token Length Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Max {field} token length: {max(token_lengths)}\")\n",
    "        print(f\"Min {field} token length: {min(token_lengths)}\")\n",
    "        print(f\"Mean {field} token length: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"Standard deviation of {field} token length: {np.std(token_lengths):.2f}\")"
   ],
   "id": "6d4f3c32f55363eb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "94d685bff580ef49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:59.173832Z",
     "start_time": "2024-11-03T08:06:58.693187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG.model_id,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "streamer = TextStreamer(tokenizer)"
   ],
   "id": "c3fe30beb7614bc8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:06:59.178979Z",
     "start_time": "2024-11-03T08:06:59.175666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.all_special_tokens}\")"
   ],
   "id": "2630dd4cb3663ac5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128000\n",
      "Special tokens: ['<|begin_of_text|>', '<|end_of_text|>']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d82f87f74b3d94e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:07:02.198440Z",
     "start_time": "2024-11-03T08:06:59.179868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG.model_id,\n",
    "    device_map=CONFIG.device,\n",
    "    attn_implementation=CONFIG.attn_implementation,\n",
    "    torch_dtype=CONFIG.torch_dtype,\n",
    "    use_cache=False\n",
    ")"
   ],
   "id": "a226fbcbda585418",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:07:02.203793Z",
     "start_time": "2024-11-03T08:07:02.199411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ],
   "id": "e7176bb1ccdeeca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Number of parameters: 1.24B\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:07:02.210153Z",
     "start_time": "2024-11-03T08:07:02.204768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    sample_text = \"Machine learning:\"\n",
    "    sample_generated_text = generate_base_model(sample_text)\n",
    "    print(sample_generated_text)"
   ],
   "id": "6a4d50069ee8697",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "7632317c22989499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:07:05.687617Z",
     "start_time": "2024-11-03T08:07:02.211208Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(CONFIG.dataset_id)",
   "id": "f6f8bf4732ed38ab",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:07:05.692325Z",
     "start_time": "2024-11-03T08:07:05.688914Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "63f155605ee4c12c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'instruction', 'output'],\n",
       "        num_rows: 60061\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:07:05.701013Z",
     "start_time": "2024-11-03T08:07:05.693381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(f\"input:\\n{dataset['train'][0]['input']}\")\n",
    "    print(f\"instruction:\\n{dataset['train'][0]['instruction']}\")\n",
    "    print(f\"output:\\n{dataset['train'][0]['output']}\")"
   ],
   "id": "eb35ca6533c912b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "귀하는 항상 설명을 제공하는 도움이 되는 조수입니다. 5살짜리 아이에게 대답한다고 생각하세요.\n",
      "instruction:\n",
      "Review:\n",
      "자비로운 속임수에 대한 씁쓸한 현대 코미디 , 영화 제작자의 시대 작품에 필적하지는 않지만 여전히 볼만한 가치가 있습니다.\n",
      "이 영화 리뷰 문장이 부정적인가요, 긍정적인가요?\n",
      "output:\n",
      "이 영화 리뷰 문장은 대부분 긍정적입니다. 리뷰어는 이 영화가 달콤함과 재미가 잘 어우러져 있으며, 감독의 다른 영화만큼 훌륭하지는 않지만 여전히 볼 만한 가치가 있다고 평가합니다.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T08:07:08.659566Z",
     "start_time": "2024-11-03T08:07:05.702007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    plot_token_length([\"input\", \"instruction\", \"output\"])"
   ],
   "id": "8ad513911b4d05e2",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m plot_token_length([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstruction\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "Cell \u001B[0;32mIn[12], line 3\u001B[0m, in \u001B[0;36mplot_token_length\u001B[0;34m(fields)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_token_length\u001B[39m(fields):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m field \u001B[38;5;129;01min\u001B[39;00m fields:\n\u001B[0;32m----> 3\u001B[0m         token_lengths \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlen\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mencode(example[field])) \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m dataset[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m example[field] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      5\u001B[0m         plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[1;32m      6\u001B[0m         plt\u001B[38;5;241m.\u001B[39mhist(token_lengths, bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mskyblue\u001B[39m\u001B[38;5;124m'\u001B[39m, edgecolor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblack\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2788\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001B[0m\n\u001B[1;32m   2750\u001B[0m \u001B[38;5;129m@add_end_docstrings\u001B[39m(\n\u001B[1;32m   2751\u001B[0m     ENCODE_KWARGS_DOCSTRING,\n\u001B[1;32m   2752\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2771\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2772\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m   2773\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2774\u001B[0m \u001B[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001B[39;00m\n\u001B[1;32m   2775\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2786\u001B[0m \u001B[38;5;124;03m            method).\u001B[39;00m\n\u001B[1;32m   2787\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2788\u001B[0m     encoded_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   2789\u001B[0m         text,\n\u001B[1;32m   2790\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[1;32m   2791\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   2792\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   2793\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   2794\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   2795\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m   2796\u001B[0m         padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m   2797\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m   2798\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2799\u001B[0m     )\n\u001B[1;32m   2801\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m encoded_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3207\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   3197\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   3198\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   3199\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3200\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3204\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3205\u001B[0m )\n\u001B[0;32m-> 3207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encode_plus(\n\u001B[1;32m   3208\u001B[0m     text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   3209\u001B[0m     text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[1;32m   3210\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   3211\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m   3212\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[1;32m   3213\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   3214\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m   3215\u001B[0m     is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m   3216\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m   3217\u001B[0m     padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m   3218\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m   3219\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m   3220\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m   3221\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m   3222\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m   3223\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m   3224\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m   3225\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m   3226\u001B[0m     split_special_tokens\u001B[38;5;241m=\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit_special_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_special_tokens),\n\u001B[1;32m   3227\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3228\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:603\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_encode_plus\u001B[39m(\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    581\u001B[0m     text: Union[TextInput, PreTokenizedInput],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    601\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchEncoding:\n\u001B[1;32m    602\u001B[0m     batched_input \u001B[38;5;241m=\u001B[39m [(text, text_pair)] \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;28;01melse\u001B[39;00m [text]\n\u001B[0;32m--> 603\u001B[0m     batched_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[1;32m    604\u001B[0m         batched_input,\n\u001B[1;32m    605\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m    606\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m    607\u001B[0m         padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m    608\u001B[0m         truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[1;32m    609\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m    610\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m    611\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m    612\u001B[0m         padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m    613\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m    614\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m    615\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m    616\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m    617\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m    618\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m    619\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m    620\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m    621\u001B[0m         split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[1;32m    622\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    623\u001B[0m     )\n\u001B[1;32m    625\u001B[0m     \u001B[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001B[39;00m\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_overflowing_tokens:\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:529\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[0m\n\u001B[1;32m    526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_special_tokens \u001B[38;5;241m!=\u001B[39m split_special_tokens:\n\u001B[1;32m    527\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_special_tokens \u001B[38;5;241m=\u001B[39m split_special_tokens\n\u001B[0;32m--> 529\u001B[0m encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenizer\u001B[38;5;241m.\u001B[39mencode_batch(\n\u001B[1;32m    530\u001B[0m     batch_text_or_text_pairs,\n\u001B[1;32m    531\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m    532\u001B[0m     is_pretokenized\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m    533\u001B[0m )\n\u001B[1;32m    535\u001B[0m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;66;03m# `Tokens` has type: Tuple[\u001B[39;00m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;66;03m#                       List[EncodingFast]\u001B[39;00m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[1;32m    541\u001B[0m tokens_and_encodings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_encoding(\n\u001B[1;32m    543\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[1;32m    553\u001B[0m ]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "46087adf588dd804"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = dataset.shuffle(seed=CONFIG.seed)",
   "id": "278c6c7409d7fa36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(formatting_func)\n",
    "dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\"])"
   ],
   "id": "de6b10a9b5f583af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[\"train\"][0][\"text\"])\n",
    "    print(dataset[\"train\"][1][\"text\"])"
   ],
   "id": "9364fbdd7cb5092",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    plot_token_length([\"text\"])"
   ],
   "id": "408da899aa43448e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = dataset[\"train\"].train_test_split(test_size=CONFIG.validation_size)",
   "id": "803cd5f967d83c64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "d6dc8a8b9232d910",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "77bb71c70f1114c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Supervised Fine-Tuning",
   "id": "a9029bed5c49bee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=CONFIG.output_dir,\n",
    "    logging_dir=CONFIG.logging_dir,\n",
    "    save_strategy=CONFIG.save_strategy,\n",
    "    logging_strategy=CONFIG.logging_strategy,\n",
    "    logging_steps=CONFIG.logging_steps,\n",
    "    evaluation_strategy=CONFIG.evaluation_strategy,\n",
    "    save_total_limit=CONFIG.save_total_limit,\n",
    "    report_to=CONFIG.report_to,\n",
    "    \n",
    "    num_train_epochs=CONFIG.num_train_epochs,\n",
    "    per_device_train_batch_size=CONFIG.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n",
    "    gradient_checkpointing=CONFIG.gradient_checkpointing,\n",
    "    bf16=CONFIG.bf16,\n",
    "    learning_rate=CONFIG.learning_rate,\n",
    "    lr_scheduler_type=CONFIG.lr_scheduler_type,\n",
    "    warmup_ratio=CONFIG.warmup_ratio,\n",
    "    optim=CONFIG.optim,\n",
    "    weight_decay=CONFIG.weight_decay,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG.max_seq_length\n",
    ")\n",
    "\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer,\n",
    "    response_template=response_template\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=collator\n",
    ")"
   ],
   "id": "d2d9843552b6ff26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "fef742b59ae750cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    trainer.save_model(CONFIG.model_name)\n",
    "    wandb.finish()"
   ],
   "id": "81f595ecc945c59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "16efd8cf8a74df79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_system = \"당신은 친절한 도우미입니다.\"\n",
    "sample_user = \"머신러닝이 무엇인가요?\"\n",
    "sample_generated_response = generate_instruct_model(sample_system, sample_user)"
   ],
   "id": "4e5c66be436793dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upload",
   "id": "9b5cf1b4ed3708c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    tokenizer.push_to_hub(\n",
    "        repo_id=CONFIG.repo_id,\n",
    "        use_temp_dir=False\n",
    "    )\n",
    "    model.push_to_hub(\n",
    "        repo_id=CONFIG.repo_id,\n",
    "        use_temp_dir=False\n",
    "    )"
   ],
   "id": "f5d7068b7d9c97cc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
