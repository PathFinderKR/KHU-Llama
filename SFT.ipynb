{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "433ddd1e8028bb07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.496553Z",
     "start_time": "2024-11-24T14:01:48.982486Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Huggingface\n",
    "import huggingface_hub\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Unsloth\n",
    "from unsloth import FastLanguageModel, FastVisionModel, is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "d35dfb5dcffb629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:07:12.139627Z",
     "start_time": "2024-11-24T14:07:12.133642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = True\n",
    "    \n",
    "    # Model\n",
    "    model_id: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    model_type: str = \"language\"  # vision | language\n",
    "    \n",
    "    # HuggingFace Hub\n",
    "    username: str = \"PathFinderKR\"\n",
    "    model_name: str = f\"KHU-Llama-3.2-3B-Instruct-SFT\"\n",
    "    \n",
    "    # Data\n",
    "    dataset_id: str = \"mlabonne/FineTome-100k\"  # \"yahma/alpaca-cleaned\"\n",
    "    dataset_template: str = \"chat\"  # alpaca | chat\n",
    "    \n",
    "    # Training\n",
    "    ## Paths\n",
    "    output_dir: str = \"./results\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 10\n",
    "    save_total_limit: int = 1\n",
    "    report_to: str = \"wandb\" if not debug else None\n",
    "    ## Hyperparameters\n",
    "    num_train_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    fp16: bool = not is_bf16_supported()\n",
    "    bf16: bool = is_bf16_supported()\n",
    "    dtype: torch.dtype = torch.bfloat16 if is_bf16_supported() else torch.float16\n",
    "    load_in_4bit: bool = True\n",
    "    learning_rate: float = 2e-5\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    optim: str = \"adamw_8bit\"\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_length: int = 2048\n",
    "    dataset_num_proc: int = 2\n",
    "    packing: bool = True\n",
    "    ### LoRA\n",
    "    lora: bool = True\n",
    "    if lora:\n",
    "        r: int = 16\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"]\n",
    "        lora_alpha: int = 32\n",
    "        lora_dropout: float = 0\n",
    "        bias: str = \"none\"\n",
    "        use_gradient_checkpointing: str = \"unsloth\"\n",
    "        use_rslora: bool = False\n",
    "        loftq_config: str = None\n",
    "        save_method: str = \"merged_16bit\"\n",
    "    \n",
    "    # Inference\n",
    "    max_new_tokens: int = 2048\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    repetition_penalty: float = 1.1\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42"
   ],
   "id": "93281c231467f7e7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "5525bda12104bb7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.519064Z",
     "start_time": "2024-11-24T14:01:54.506576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "c7f89213fe1e3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "7844e5ceef3bb8a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.524392Z",
     "start_time": "2024-11-24T14:01:54.520841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "b24f8a736bfe545f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Debugging",
   "id": "8fadfa0dd0dd4538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.531390Z",
     "start_time": "2024-11-24T14:01:54.525448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.num_train_epochs = 1"
   ],
   "id": "214b07b072f88149",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HuggingFace",
   "id": "c148b23c553f1ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.850608Z",
     "start_time": "2024-11-24T14:01:54.532635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "huggingface_hub.login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "id": "67bd0191d59a818f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weights & Biases",
   "id": "bd1cd43b2bdb24d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.854350Z",
     "start_time": "2024-11-24T14:01:54.851736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.login(\n",
    "        key=os.getenv(\"WANDB_API_KEY\")\n",
    "    )\n",
    "    wandb.init(\n",
    "        project=CONFIG.model_name\n",
    "    )"
   ],
   "id": "9c1d79090b69737f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utility Functions",
   "id": "295cb5ac174c88b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:03:16.600144Z",
     "start_time": "2024-11-24T14:03:16.596006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "llama_3_instruct_prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "# Formatting functions\n",
    "def apply_alpaca_template(examples):\n",
    "    texts = []\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = tokenizer.bos_token + alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def apply_llama_template(examples):\n",
    "    texts = []\n",
    "    for conversation in examples:\n",
    "        system = \"\"\n",
    "        user = \"\"\n",
    "        assistant = \"\"\n",
    "        for message in conversation:\n",
    "            if message['from'] == 'system':\n",
    "                system = message['value']\n",
    "            elif message['from'] == 'human':\n",
    "                user = message['value']\n",
    "            elif message['from'] == 'gpt':\n",
    "                assistant = message['value']\n",
    "        if CONFIG.model_type == \"language\":\n",
    "            text = tokenizer.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + tokenizer.eos_token\n",
    "        elif CONFIG.model_type == \"vision\":\n",
    "            text = processor.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + processor.eos_token\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ],
   "id": "f7ff6c3caa4b6336",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.872675Z",
     "start_time": "2024-11-24T14:01:54.863373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate base model\n",
    "def generate_text(prompt):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "# Generate instruction model\n",
    "def generate_response(system, user):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        llama_3_instruct_prompt.format(\n",
    "            system,\n",
    "            user,\n",
    "            \"\"\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "# Generate vision model\n",
    "def generate_vision(system, user):\n",
    "    FastVisionModel.for_inference(model)\n",
    "    input_text = [\n",
    "        llama_3_instruct_prompt.format(\n",
    "            system,\n",
    "            user,\n",
    "            \"\"\n",
    "        )\n",
    "    ]\n",
    "    inputs = processor(\n",
    "        images=None,\n",
    "        texts=input_text,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ],
   "id": "9367124b154dabf9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:01:54.882316Z",
     "start_time": "2024-11-24T14:01:54.874322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_token_length(fields):\n",
    "    for field in fields:\n",
    "        token_lengths = [len(tokenizer.encode(example[field])) for example in dataset if example[field] != \"\"]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel(f'{field.capitalize()} Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{field.capitalize()} Token Length Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Max {field} token length: {max(token_lengths)}\")\n",
    "        print(f\"Min {field} token length: {min(token_lengths)}\")\n",
    "        print(f\"Mean {field} token length: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"Standard deviation of {field} token length: {np.std(token_lengths):.2f}\")"
   ],
   "id": "da41a5bf2f6a36f0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d82f87f74b3d94e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:02:06.356485Z",
     "start_time": "2024-11-24T14:01:54.883505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.model_type == \"language\":\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=CONFIG.model_id,\n",
    "        max_seq_length=CONFIG.max_seq_length,\n",
    "        dtype=CONFIG.dtype,\n",
    "        load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    "    )\n",
    "elif CONFIG.model_type == \"vision\":\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name=CONFIG.model_id,\n",
    "        max_seq_length=CONFIG.max_seq_length,\n",
    "        dtype=CONFIG.dtype,\n",
    "        load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid model type\")"
   ],
   "id": "cb1e60af7efcb450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.8: Fast Llama patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 SUPER. Max memory: 15.992 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:02:06.359660Z",
     "start_time": "2024-11-24T14:02:06.357618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "#print(f\"Special tokens: {tokenizer.all_special_tokens}\")"
   ],
   "id": "2630dd4cb3663ac5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:02:06.369019Z",
     "start_time": "2024-11-24T14:02:06.360834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ],
   "id": "e7176bb1ccdeeca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ")\n",
      "Number of parameters: 3.21B\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:02:07.322648Z",
     "start_time": "2024-11-24T14:02:06.370116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    sample_system = \"You are a helpful assistant.\"\n",
    "    sample_user = \"What is the capital of France?\"\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        sample_response = generate_response(sample_system, sample_user)\n",
    "        print(sample_response)\n",
    "        #print(tokenizer.tokenize(sample_response[0]))\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        sample_response = generate_vision(sample_system, sample_user)\n",
    "        print(sample_response)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")"
   ],
   "id": "aa3bb022dea115b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of France is Paris.<|eot_id|>\n",
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe capital of France is Paris.<|eot_id|>']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "7632317c22989499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:07:23.193622Z",
     "start_time": "2024-11-24T14:07:19.368311Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(CONFIG.dataset_id, split=\"train\")",
   "id": "f6f8bf4732ed38ab",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:02:07.905282Z",
     "start_time": "2024-11-24T14:02:07.327121Z"
    }
   },
   "cell_type": "code",
   "source": "#dataset = load_dataset('json', data_files='KUHrious_SFT_Dataset_transformed.jsonl', split='train')",
   "id": "163266298433ee4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:07:45.497718Z",
     "start_time": "2024-11-24T14:07:45.494128Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "63f155605ee4c12c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'source', 'score'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:06:13.440290Z",
     "start_time": "2024-11-24T14:06:13.436379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    if CONFIG.dataset_template == \"alpaca\":\n",
    "        print(f\"instruction: {dataset[0]['instruction']}\")\n",
    "        print(f\"input: {dataset[0]['input']}\")\n",
    "        print(f\"output: {dataset[0]['output']}\")\n",
    "    elif CONFIG.dataset_template == \"chat\":\n",
    "        #print(f\"conversations: {dataset[0]['features']}\")\n",
    "        #print(f\"source: {dataset[0]['source']}\")\n",
    "        #print(f\"score: {dataset[0]['score']}\")\n",
    "        print(dataset[0])\n",
    "        print(dataset[1])"
   ],
   "id": "50e594b242b76d34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 'human', 'value': 'What are the key dates for course registration in the first semester of 2024?', 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the key dates for course registration in the first semester of 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<|eot_id|>'}\n",
      "{'from': 'gpt', 'value': 'The key dates for course registration in the first semester of 2024 are as follows:\\\\n\\\\n1. **Course Registration Confirmation and Changes:** March 4 to March 8, 2024. During this period, students can confirm their course selections and make any necessary changes.\\\\n2. **Credit Withdrawal Applications:** This is also available from March 4 to March 8, allowing students to withdraw from courses without penalty.\\\\n3. **Double Majors and Minors Application Period:** From March 11 to March 13, 2024, students can apply for double majors and minors.\\\\n4. **Course Withdrawal Applications:** From March 25 to March 29, 2024, students can formally withdraw from courses they are enrolled in.\\\\n5. **Major Entry Grade Evaluation Center Applications:** This application is available during the same period as course withdrawal (March 25 to March 29, 2024). \\\\n\\\\nThese dates are crucial for students to manage their academic paths effectively.', 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe key dates for course registration in the first semester of 2024 are as follows:\\\\n\\\\n1. **Course Registration Confirmation and Changes:** March 4 to March 8, 2024. During this period, students can confirm their course selections and make any necessary changes.\\\\n2. **Credit Withdrawal Applications:** This is also available from March 4 to March 8, allowing students to withdraw from courses without penalty.\\\\n3. **Double Majors and Minors Application Period:** From March 11 to March 13, 2024, students can apply for double majors and minors.\\\\n4. **Course Withdrawal Applications:** From March 25 to March 29, 2024, students can formally withdraw from courses they are enrolled in.\\\\n5. **Major Entry Grade Evaluation Center Applications:** This application is available during the same period as course withdrawal (March 25 to March 29, 2024). \\\\n\\\\nThese dates are crucial for students to manage their academic paths effectively.<|eot_id|>'}\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "333975d5c50820a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:05:48.100887Z",
     "start_time": "2024-11-24T14:05:48.097362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_llama_template(examples):\n",
    "    texts = []\n",
    "    for from_value, value in zip(examples['from'], examples['value']):\n",
    "        system = \"\"\n",
    "        user = \"\"\n",
    "        assistant = \"\"\n",
    "        \n",
    "        if from_value == 'system':\n",
    "            system = value\n",
    "        elif from_value == 'human':\n",
    "            user = value\n",
    "        elif from_value == 'gpt':\n",
    "            assistant = value\n",
    "        \n",
    "        if CONFIG.model_type == \"language\":\n",
    "            text = tokenizer.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + tokenizer.eos_token\n",
    "        elif CONFIG.model_type == \"vision\":\n",
    "            text = processor.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + processor.eos_token\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "        \n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ],
   "id": "efe038f572422e74",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:05:50.464204Z",
     "start_time": "2024-11-24T14:05:50.266392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.dataset_template == \"alpaca\":\n",
    "    formatting_func = apply_alpaca_template\n",
    "elif CONFIG.dataset_template == \"chat\":\n",
    "    formatting_func = apply_llama_template\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset template\")\n",
    "\n",
    "dataset = dataset.map(formatting_func, batched=True)"
   ],
   "id": "898878d6328ab66b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/13986 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f596beaca314f84a5f0c2c5d8ee171c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T14:05:52.179091Z",
     "start_time": "2024-11-24T14:05:52.175562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[0][\"text\"])\n",
    "    #print(tokenizer.tokenize(dataset[0][\"text\"]))"
   ],
   "id": "eb35ca6533c912b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the key dates for course registration in the first semester of 2024?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    plot_token_length([\"text\"])"
   ],
   "id": "e2569cf935d78df9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Supervised Fine-Tuning (LoRA)",
   "id": "a9029bed5c49bee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=CONFIG.r,\n",
    "            target_modules=CONFIG.target_modules,\n",
    "            lora_alpha=CONFIG.lora_alpha,\n",
    "            lora_dropout=CONFIG.lora_dropout,\n",
    "            bias=CONFIG.bias,\n",
    "            use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "            use_rslora=CONFIG.use_rslora,\n",
    "            loftq_config=CONFIG.loftq_config,\n",
    "            random_state=CONFIG.seed\n",
    "        )\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        model = FastVisionModel.get_peft_model(\n",
    "            model,\n",
    "            finetune_vision_layers     = False, # False if not finetuning vision layers\n",
    "            finetune_language_layers   = True, # False if not finetuning language layers\n",
    "            finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "            finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "            \n",
    "            r=CONFIG.r,\n",
    "            target_modules=CONFIG.target_modules,\n",
    "            lora_alpha=CONFIG.lora_alpha,\n",
    "            lora_dropout=CONFIG.lora_dropout,\n",
    "            bias=CONFIG.bias,\n",
    "            use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "            use_rslora=CONFIG.use_rslora,\n",
    "            loftq_config=CONFIG.loftq_config,\n",
    "            random_state=CONFIG.seed\n",
    "        )"
   ],
   "id": "41dbb68405aa52b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    model.print_trainable_parameters()"
   ],
   "id": "f005f3823824359e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG.max_seq_length,\n",
    "    dataset_num_proc=CONFIG.dataset_num_proc,\n",
    "    packing=CONFIG.packing,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer) if CONFIG.model_type == \"vision\" else None,\n",
    "    args=SFTConfig(\n",
    "        output_dir=CONFIG.output_dir,\n",
    "        logging_dir=CONFIG.logging_dir,\n",
    "        save_strategy=CONFIG.save_strategy,\n",
    "        logging_strategy=CONFIG.logging_strategy,\n",
    "        logging_steps=CONFIG.logging_steps,\n",
    "        save_total_limit=CONFIG.save_total_limit,\n",
    "        report_to=CONFIG.report_to,\n",
    "        num_train_epochs=CONFIG.num_train_epochs,\n",
    "        per_device_train_batch_size=CONFIG.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n",
    "        fp16=CONFIG.fp16,\n",
    "        bf16=CONFIG.bf16,\n",
    "        learning_rate=CONFIG.learning_rate,\n",
    "        lr_scheduler_type=CONFIG.lr_scheduler_type,\n",
    "        warmup_ratio=CONFIG.warmup_ratio,\n",
    "        optim=CONFIG.optim,\n",
    "        weight_decay=CONFIG.weight_decay\n",
    "    )\n",
    ")"
   ],
   "id": "d2d9843552b6ff26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "fef742b59ae750cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.finish()\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "        tokenizer.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "    else:\n",
    "        model.save_pretrained(CONFIG.model_name)\n",
    "        tokenizer.save_pretrained(CONFIG.model_name)"
   ],
   "id": "81f595ecc945c59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "b0bc305346d4050e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    sample_system = \"You are a helpful assistant.\"\n",
    "    sample_user = \"What is the capital of France?\"\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        sample_response = generate_response(sample_system, sample_user)\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        sample_response = generate_vision(sample_system, sample_user)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "    print(sample_response)\n",
    "    print(tokenizer.tokenize(sample_response[0]))"
   ],
   "id": "b7b3749dec022836",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save",
   "id": "9b5cf1b4ed3708c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "        model.push_to_hub_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "    else:\n",
    "        model.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )"
   ],
   "id": "a4c00c867c11af7d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
