{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "433ddd1e8028bb07"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.195640Z",
     "start_time": "2024-11-24T12:56:14.064635Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Huggingface\n",
    "import huggingface_hub\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Unsloth\n",
    "from unsloth import FastLanguageModel, FastVisionModel, is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "d35dfb5dcffb629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.204425Z",
     "start_time": "2024-11-24T12:56:20.197105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = True\n",
    "    \n",
    "    # Model\n",
    "    model_id: str = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    model_type: str = \"language\"  # vision | language\n",
    "    \n",
    "    # HuggingFace Hub\n",
    "    username: str = \"PathFinderKR\"\n",
    "    model_name: str = f\"KHU-Llama-3.2-11B-Vision-Instruct\"\n",
    "    \n",
    "    # Data\n",
    "    dataset_id: str = \"mlabonne/FineTome-100k\"  # \"yahma/alpaca-cleaned\"\n",
    "    dataset_template: str = \"chat\"  # alpaca | chat\n",
    "    \n",
    "    # Training\n",
    "    ## Paths\n",
    "    output_dir: str = \"./results\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 10\n",
    "    save_total_limit: int = 1\n",
    "    report_to: str = \"wandb\" if not debug else None\n",
    "    ## Hyperparameters\n",
    "    num_train_epochs: int = 1\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    fp16: bool = not is_bf16_supported()\n",
    "    bf16: bool = is_bf16_supported()\n",
    "    dtype: torch.dtype = torch.bfloat16 if is_bf16_supported() else torch.float16\n",
    "    load_in_4bit: bool = True\n",
    "    learning_rate: float = 2e-5\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    optim: str = \"adamw_8bit\"\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_length: int = 2048\n",
    "    dataset_num_proc: int = 2\n",
    "    packing: bool = True\n",
    "    ### LoRA\n",
    "    lora: bool = True\n",
    "    if lora:\n",
    "        r: int = 16\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"]\n",
    "        lora_alpha: int = 32\n",
    "        lora_dropout: float = 0\n",
    "        bias: str = \"none\"\n",
    "        use_gradient_checkpointing: str = \"unsloth\"\n",
    "        use_rslora: bool = False\n",
    "        loftq_config: str = None\n",
    "        save_method: str = \"merged_16bit\"\n",
    "    \n",
    "    # Inference\n",
    "    max_new_tokens: int = 2048\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    repetition_penalty: float = 1.1\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42"
   ],
   "id": "93281c231467f7e7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "5525bda12104bb7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.218391Z",
     "start_time": "2024-11-24T12:56:20.205674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "c7f89213fe1e3b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "7844e5ceef3bb8a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.223375Z",
     "start_time": "2024-11-24T12:56:20.219907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "b24f8a736bfe545f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Debugging",
   "id": "8fadfa0dd0dd4538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.230006Z",
     "start_time": "2024-11-24T12:56:20.224348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.num_train_epochs = 1"
   ],
   "id": "214b07b072f88149",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HuggingFace",
   "id": "c148b23c553f1ff3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.495563Z",
     "start_time": "2024-11-24T12:56:20.231202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "huggingface_hub.login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "id": "67bd0191d59a818f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weights & Biases",
   "id": "bd1cd43b2bdb24d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.499299Z",
     "start_time": "2024-11-24T12:56:20.496800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.login(\n",
    "        key=os.getenv(\"WANDB_API_KEY\")\n",
    "    )\n",
    "    wandb.init(\n",
    "        project=CONFIG.model_name\n",
    "    )"
   ],
   "id": "9c1d79090b69737f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utility Functions",
   "id": "295cb5ac174c88b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:56:20.507251Z",
     "start_time": "2024-11-24T12:56:20.500265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "llama_3_instruct_prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "# Formatting functions\n",
    "def apply_alpaca_template(examples):\n",
    "    texts = []\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = tokenizer.bos_token + alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def apply_llama_template(examples):\n",
    "    texts = []\n",
    "    for conversation in examples['conversations']:\n",
    "        system = \"\"\n",
    "        user = \"\"\n",
    "        assistant = \"\"\n",
    "        for message in conversation:\n",
    "            if message['from'] == 'system':\n",
    "                system = message['value']\n",
    "            elif message['from'] == 'human':\n",
    "                user = message['value']\n",
    "            elif message['from'] == 'gpt':\n",
    "                assistant = message['value']\n",
    "        if CONFIG.model_type == \"language\":\n",
    "            text = tokenizer.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + tokenizer.eos_token\n",
    "        elif CONFIG.model_type == \"vision\":\n",
    "            text = processor.bos_token + llama_3_instruct_prompt.format(system, user, assistant) + processor.eos_token\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type\")\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ],
   "id": "f7ff6c3caa4b6336",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:59:00.035179Z",
     "start_time": "2024-11-24T12:59:00.029780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate base model\n",
    "def generate_text(prompt):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "# Generate instruction model\n",
    "def generate_response(system, user):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        llama_3_instruct_prompt.format(\n",
    "            system,\n",
    "            user,\n",
    "            \"\"\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "# Generate vision model\n",
    "def generate_vision(system, user):\n",
    "    FastVisionModel.for_inference(model)\n",
    "    input_text = [\n",
    "        llama_3_instruct_prompt.format(\n",
    "            system,\n",
    "            user,\n",
    "            \"\"\n",
    "        )\n",
    "    ]\n",
    "    inputs = processor(\n",
    "        images=None,\n",
    "        texts=input_text,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(CONFIG.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=CONFIG.max_new_tokens,\n",
    "        do_sample=CONFIG.do_sample,\n",
    "        temperature=CONFIG.temperature,\n",
    "        top_p=CONFIG.top_p,\n",
    "        repetition_penalty=CONFIG.repetition_penalty,\n",
    "        use_cache=True,\n",
    "        streamer=TextStreamer(tokenizer)\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ],
   "id": "9367124b154dabf9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:59:00.406856Z",
     "start_time": "2024-11-24T12:59:00.402955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_token_length(fields):\n",
    "    for field in fields:\n",
    "        token_lengths = [len(tokenizer.encode(example[field])) for example in dataset if example[field] != \"\"]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel(f'{field.capitalize()} Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'{field.capitalize()} Token Length Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Max {field} token length: {max(token_lengths)}\")\n",
    "        print(f\"Min {field} token length: {min(token_lengths)}\")\n",
    "        print(f\"Mean {field} token length: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"Standard deviation of {field} token length: {np.std(token_lengths):.2f}\")"
   ],
   "id": "da41a5bf2f6a36f0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "d82f87f74b3d94e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:59:20.244943Z",
     "start_time": "2024-11-24T12:59:00.741477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.model_type == \"language\":\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=CONFIG.model_id,\n",
    "        max_seq_length=CONFIG.max_seq_length,\n",
    "        dtype=CONFIG.dtype,\n",
    "        load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    "    )\n",
    "elif CONFIG.model_type == \"vision\":\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name=CONFIG.model_id,\n",
    "        max_seq_length=CONFIG.max_seq_length,\n",
    "        dtype=CONFIG.dtype,\n",
    "        load_in_4bit=CONFIG.load_in_4bit if CONFIG.lora else False\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid model type\")"
   ],
   "id": "cb1e60af7efcb450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.8: Fast Mllama vision patching. Transformers = 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 SUPER. Max memory: 15.992 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 8.9. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35ec9d093164485e97cbd7664970d768"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:59:20.248668Z",
     "start_time": "2024-11-24T12:59:20.246197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "#print(f\"Special tokens: {tokenizer.all_special_tokens}\")"
   ],
   "id": "2630dd4cb3663ac5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:59:20.262550Z",
     "start_time": "2024-11-24T12:59:20.249544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_parameters() / 1e9:.2f}B\")"
   ],
   "id": "e7176bb1ccdeeca4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MllamaForConditionalGeneration(\n",
      "  (vision_model): MllamaVisionModel(\n",
      "    (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n",
      "    (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n",
      "      (tile_embedding): Embedding(9, 8197120)\n",
      "    )\n",
      "    (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
      "      (embedding): Embedding(9, 5120)\n",
      "    )\n",
      "    (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
      "      (embedding): Embedding(9, 5120)\n",
      "    )\n",
      "    (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    (transformer): MllamaVisionEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x MllamaVisionEncoderLayer(\n",
      "          (self_attn): MllamaVisionSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaVisionMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "            (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (global_transformer): MllamaVisionEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-7): 8 x MllamaVisionEncoderLayer(\n",
      "          (self_attn): MllamaVisionSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaVisionMLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "            (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (language_model): MllamaForCausalLM(\n",
      "    (model): MllamaTextModel(\n",
      "      (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (3): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (8): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (13): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (18): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (23): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (28): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (33): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (38): MllamaCrossAttentionDecoderLayer(\n",
      "          (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "        (39): MllamaSelfAttentionDecoderLayer(\n",
      "          (self_attn): MllamaTextSelfSdpaAttention(\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): MllamaTextMLP(\n",
      "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "      (rotary_emb): MllamaRotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "  )\n",
      "  (multi_modal_projector): Linear4bit(in_features=7680, out_features=4096, bias=True)\n",
      ")\n",
      "Number of parameters: 10.67B\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T12:59:20.300019Z",
     "start_time": "2024-11-24T12:59:20.264118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    sample_system = \"You are a helpful assistant.\"\n",
    "    sample_user = \"What is the capital of France?\"\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        sample_response = generate_response(sample_system, sample_user)\n",
    "        print(sample_response)\n",
    "        #print(tokenizer.tokenize(sample_response[0]))\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        sample_response = generate_vision(sample_system, sample_user)\n",
    "        print(sample_response)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")"
   ],
   "id": "aa3bb022dea115b9",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must specify either text or images.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sample_response[\u001B[38;5;241m0\u001B[39m]))\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m CONFIG\u001B[38;5;241m.\u001B[39mmodel_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvision\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 9\u001B[0m     sample_response \u001B[38;5;241m=\u001B[39m generate_vision(sample_system, sample_user)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(sample_response)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[16], line 53\u001B[0m, in \u001B[0;36mgenerate_vision\u001B[0;34m(system, user)\u001B[0m\n\u001B[1;32m     45\u001B[0m FastVisionModel\u001B[38;5;241m.\u001B[39mfor_inference(model)\n\u001B[1;32m     46\u001B[0m input_text \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     47\u001B[0m     llama_3_instruct_prompt\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m     48\u001B[0m         system,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m ]\n\u001B[0;32m---> 53\u001B[0m inputs \u001B[38;5;241m=\u001B[39m processor(\n\u001B[1;32m     54\u001B[0m     images\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     55\u001B[0m     texts\u001B[38;5;241m=\u001B[39minput_text,\n\u001B[1;32m     56\u001B[0m     return_tensors \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     57\u001B[0m )\u001B[38;5;241m.\u001B[39mto(CONFIG\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     58\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs,\n\u001B[1;32m     60\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39mCONFIG\u001B[38;5;241m.\u001B[39mmax_new_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     66\u001B[0m     streamer\u001B[38;5;241m=\u001B[39mTextStreamer(tokenizer)\n\u001B[1;32m     67\u001B[0m )\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(outputs, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/transformers/models/mllama/processing_mllama.py:264\u001B[0m, in \u001B[0;36mMllamaProcessor.__call__\u001B[0;34m(self, images, text, audio, videos, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03mMain method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03marguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;124;03m    TODO: add aspect_ratio_ids and aspect_ratio_mask and cross_attention_mask\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m images \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou must specify either text or images.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    266\u001B[0m output_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_kwargs(\n\u001B[1;32m    267\u001B[0m     MllamaProcessorKwargs,\n\u001B[1;32m    268\u001B[0m     tokenizer_init_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39minit_kwargs,\n\u001B[1;32m    269\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    270\u001B[0m )\n\u001B[1;32m    272\u001B[0m text_kwargs \u001B[38;5;241m=\u001B[39m output_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mValueError\u001B[0m: You must specify either text or images."
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "7632317c22989499"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = load_dataset(CONFIG.dataset_id, split=\"train\")",
   "id": "f6f8bf4732ed38ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "63f155605ee4c12c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    if CONFIG.dataset_template == \"alpaca\":\n",
    "        print(f\"instruction: {dataset[0]['instruction']}\")\n",
    "        print(f\"input: {dataset[0]['input']}\")\n",
    "        print(f\"output: {dataset[0]['output']}\")\n",
    "    elif CONFIG.dataset_template == \"chat\":\n",
    "        print(f\"conversations: {dataset[0]['conversations']}\")\n",
    "        print(f\"source: {dataset[0]['source']}\")\n",
    "        print(f\"score: {dataset[0]['score']}\")"
   ],
   "id": "50e594b242b76d34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "333975d5c50820a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.dataset_template == \"alpaca\":\n",
    "    formatting_func = apply_alpaca_template\n",
    "elif CONFIG.dataset_template == \"chat\":\n",
    "    formatting_func = apply_llama_template\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset template\")\n",
    "\n",
    "dataset = dataset.map(formatting_func, batched=True)"
   ],
   "id": "898878d6328ab66b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    print(dataset[0][\"text\"])\n",
    "    #print(tokenizer.tokenize(dataset[0][\"text\"]))"
   ],
   "id": "eb35ca6533c912b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    plot_token_length([\"text\"])"
   ],
   "id": "e2569cf935d78df9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Supervised Fine-Tuning (LoRA)",
   "id": "a9029bed5c49bee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=CONFIG.r,\n",
    "            target_modules=CONFIG.target_modules,\n",
    "            lora_alpha=CONFIG.lora_alpha,\n",
    "            lora_dropout=CONFIG.lora_dropout,\n",
    "            bias=CONFIG.bias,\n",
    "            use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "            use_rslora=CONFIG.use_rslora,\n",
    "            loftq_config=CONFIG.loftq_config,\n",
    "            random_state=CONFIG.seed\n",
    "        )\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        model = FastVisionModel.get_peft_model(\n",
    "            model,\n",
    "            finetune_vision_layers     = False, # False if not finetuning vision layers\n",
    "            finetune_language_layers   = True, # False if not finetuning language layers\n",
    "            finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "            finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "            \n",
    "            r=CONFIG.r,\n",
    "            target_modules=CONFIG.target_modules,\n",
    "            lora_alpha=CONFIG.lora_alpha,\n",
    "            lora_dropout=CONFIG.lora_dropout,\n",
    "            bias=CONFIG.bias,\n",
    "            use_gradient_checkpointing=CONFIG.use_gradient_checkpointing,\n",
    "            use_rslora=CONFIG.use_rslora,\n",
    "            loftq_config=CONFIG.loftq_config,\n",
    "            random_state=CONFIG.seed\n",
    "        )"
   ],
   "id": "41dbb68405aa52b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    model.print_trainable_parameters()"
   ],
   "id": "f005f3823824359e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG.max_seq_length,\n",
    "    dataset_num_proc=CONFIG.dataset_num_proc,\n",
    "    packing=CONFIG.packing,\n",
    "    data_collator=UnslothVisionDataCollator(model, tokenizer) if CONFIG.model_type == \"vision\" else None,\n",
    "    args=SFTConfig(\n",
    "        output_dir=CONFIG.output_dir,\n",
    "        logging_dir=CONFIG.logging_dir,\n",
    "        save_strategy=CONFIG.save_strategy,\n",
    "        logging_strategy=CONFIG.logging_strategy,\n",
    "        logging_steps=CONFIG.logging_steps,\n",
    "        save_total_limit=CONFIG.save_total_limit,\n",
    "        report_to=CONFIG.report_to,\n",
    "        num_train_epochs=CONFIG.num_train_epochs,\n",
    "        per_device_train_batch_size=CONFIG.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n",
    "        fp16=CONFIG.fp16,\n",
    "        bf16=CONFIG.bf16,\n",
    "        learning_rate=CONFIG.learning_rate,\n",
    "        lr_scheduler_type=CONFIG.lr_scheduler_type,\n",
    "        warmup_ratio=CONFIG.warmup_ratio,\n",
    "        optim=CONFIG.optim,\n",
    "        weight_decay=CONFIG.weight_decay\n",
    "    )\n",
    ")"
   ],
   "id": "d2d9843552b6ff26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "fef742b59ae750cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    wandb.finish()\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "        tokenizer.save_pretrained(CONFIG.model_name + \"-LoRA\")\n",
    "    else:\n",
    "        model.save_pretrained(CONFIG.model_name)\n",
    "        tokenizer.save_pretrained(CONFIG.model_name)"
   ],
   "id": "81f595ecc945c59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "b0bc305346d4050e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if CONFIG.lora:\n",
    "    sample_system = \"You are a helpful assistant.\"\n",
    "    sample_user = \"What is the capital of France?\"\n",
    "    if CONFIG.model_type == \"language\":\n",
    "        sample_response = generate_response(sample_system, sample_user)\n",
    "    elif CONFIG.model_type == \"vision\":\n",
    "        sample_response = generate_vision(sample_system, sample_user)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "    print(sample_response)\n",
    "    print(tokenizer.tokenize(sample_response[0]))"
   ],
   "id": "b7b3749dec022836",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save",
   "id": "9b5cf1b4ed3708c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    if CONFIG.lora:\n",
    "        model.save_pretrained_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "        model.push_to_hub_merged(\n",
    "            CONFIG.model_name,\n",
    "            tokenizer,\n",
    "            save_method=CONFIG.save_method\n",
    "        )\n",
    "    else:\n",
    "        model.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            repo_id=CONFIG.username + \"/\" + CONFIG.model_name,\n",
    "            use_temp_dir=False\n",
    "        )"
   ],
   "id": "a4c00c867c11af7d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
